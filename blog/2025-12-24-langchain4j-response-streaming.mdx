---
slug: langchain4j-response-streaming
title: LangChain4j 流式响应
authors: [susamlu]
tags: [langchain4j, java, llm]
---

传统文本生成模式下，需等待模型生成完整响应后再整体返回，容易产生显著延迟，生成长文本时该问题尤为突出。流式响应（Response Streaming）采用逐令牌（token）传输机制，用户无需等待全量结果即可即时获取内容，交互体验得以大幅提升。LangChain4j 提供 `StreamingChatModel` 核心接口，原生支持响应流式传输。本文将围绕流式响应展开具体探讨。

<!-- truncate -->

:::info 版本说明
本文基于 **LangChain4j 1.10.0** 版本编写，API 接口与行为特性可能与其他版本存在差异，请以实际使用的版本为准。
:::

## 1. 接口抽象

LangChain4j 为流式响应提供了专门的接口抽象：

- **`StreamingChatModel`**：`ChatModel` 的流式版本，支持流式对话响应。
- **`StreamingLanguageModel`**：`LanguageModel` 的流式版本，支持流式文本生成。（`LanguageModel` 已不再推荐使用，因此本文主要针对 `StreamingChatModel` 进行展开。）

两个接口都接受 `StreamingChatResponseHandler` 的实现作为参数，用于处理流式响应过程中的各种事件。

## 2. StreamingChatResponseHandler

`StreamingChatResponseHandler` 是处理流式响应的核心接口，定义了以下回调方法：

- **`onPartialResponse(String partialResponse)`**：模型生成增量文本响应片段时触发调用，每接收到一段增量文本即执行一次。
- **`onPartialThinking(PartialThinking partialThinking)`**：模型生成下一段增量思考/推理文本时触发调用（仅部分模型支持，如 DeepSeek）。
- **`onPartialToolCall(PartialToolCall partialToolCall)`**：模型生成下一段增量工具调用内容时触发调用。
- **`onCompleteToolCall(CompleteToolCall completeToolCall)`**：单个工具调用的流式传输全部完成时触发调用。
- **`onCompleteResponse(ChatResponse completeResponse)`**：整个流式响应生成完毕时触发调用，`ChatResponse` 对象包含完整响应内容及元数据（如 token 消耗、响应结束原因等）。
- **`onError(Throwable error)`**：流式响应过程中发生异常时触发调用，可用于处理超时、限流、网络异常等各类错误场景。

:::tip 兼容性说明
不同模型厂商对上述回调方法的支持程度可能存在差异。例如，`onPartialThinking` 仅部分模型支持（如 DeepSeek），而 `onPartialToolCall` 和 `onCompleteToolCall` 仅在工具调用场景下才会触发。实际使用时请参考对应厂商的文档说明。
:::

以下是一个具体的示例代码：

<details>
<summary>StreamingChatResponseHandler 示例</summary>

```java
import dev.langchain4j.model.chat.StreamingChatModel;
import dev.langchain4j.model.chat.response.ChatResponse;
import dev.langchain4j.model.chat.response.StreamingChatResponseHandler;
import dev.langchain4j.model.openai.OpenAiStreamingChatModel;

import java.util.concurrent.CountDownLatch;

public class StreamingChatExample {

    public static void main(String[] args) throws InterruptedException {
        String apiKey = System.getenv("DEEPSEEK_API_KEY");

        StreamingChatModel model = OpenAiStreamingChatModel.builder()
                .baseUrl("https://api.deepseek.com/v1")
                .apiKey(apiKey)
                .modelName("deepseek-chat")
                .build();

        String userMessage = "给我讲一个笑话";

        // 创建 CountDownLatch，初始计数为 1
        CountDownLatch latch = new CountDownLatch(1);

        model.chat(userMessage, new StreamingChatResponseHandler() {
            @Override
            public void onPartialResponse(String partialResponse) {
                // 实时输出增量 token
                System.out.print(partialResponse);
            }

            @Override
            public void onCompleteResponse(ChatResponse completeResponse) {
                // 生成完成，打印完整响应和元数据
                System.out.println("\n\n--- 生成完成 ---");
                System.out.println("完整响应: " + completeResponse.aiMessage().text());
                System.out.println("Token 消耗: " + completeResponse.tokenUsage());
                // 响应完成，释放锁
                latch.countDown();
            }

            @Override
            public void onError(Throwable error) {
                // 错误处理
                System.err.println("发生错误: " + error.getMessage());
                // 发生错误时也要释放锁，避免主线程永久阻塞
                latch.countDown();
            }
        });

        // 阻塞主线程，等待流式响应完成
        latch.await();
    }

}
```

</details>

## 3. LambdaStreamingResponseHandler

在实际开发中，如果只需要处理部分响应和错误，使用完整的 `StreamingChatResponseHandler` 接口可能会显得过于繁琐。为此，LangChain4j 提供了 `LambdaStreamingResponseHandler` 工具类来简化代码。该类提供了静态方法，支持使用 Lambda 表达式创建 `StreamingChatResponseHandler`，避免编写冗长的匿名内部类。

`LambdaStreamingResponseHandler` 提供了以下静态方法：

- **`onPartialResponse(Consumer<String> partialResponseHandler)`**：仅处理增量响应，使用 Lambda 表达式定义处理逻辑。
- **`onPartialResponseAndError(Consumer<String> partialResponseHandler, Consumer<Throwable> errorHandler)`**：同时处理增量响应和错误，分别使用两个 Lambda 表达式定义处理逻辑。

以下是一个使用 `LambdaStreamingResponseHandler` 的简单示例：

<details>
<summary>LambdaStreamingResponseHandler 示例</summary>

```java
import dev.langchain4j.model.chat.StreamingChatModel;
import dev.langchain4j.model.openai.OpenAiStreamingChatModel;

import static dev.langchain4j.model.LambdaStreamingResponseHandler.onPartialResponse;
import static dev.langchain4j.model.LambdaStreamingResponseHandler.onPartialResponseAndError;

public class LambdaStreamingExample {

    public static void main(String[] args) throws InterruptedException {
        String apiKey = System.getenv("DEEPSEEK_API_KEY");

        StreamingChatModel model = OpenAiStreamingChatModel.builder()
                .baseUrl("https://api.deepseek.com/v1")
                .apiKey(apiKey)
                .modelName("deepseek-chat")
                .build();

        String userMessage = "给我讲一个笑话";

        // 方式一：仅处理增量响应
        System.out.println("【开始流式输出】");
        model.chat(userMessage, onPartialResponse(System.out::print));
        Thread.sleep(10000); // 等待流式响应完成

        // 方式二：同时处理增量响应和错误
        System.out.println("\n\n【开始流式输出】");
        model.chat(userMessage, onPartialResponseAndError(
                System.out::print,                    // 处理增量响应
                Throwable::printStackTrace            // 处理错误
        ));
        Thread.sleep(10000); // 等待流式响应完成
    }

}
```

</details>

:::tip 注意事项
`LambdaStreamingResponseHandler` 适用于简单流式输出、快速原型开发和日志记录等场景。它仅提供了 `onPartialResponse` 和 `onError` 的简化方法，如果需要处理 `onCompleteResponse`、`onPartialThinking` 或 `onCompleteToolCall` 等事件，仍需使用完整的 `StreamingChatResponseHandler` 实现。
:::

## 4. 流式任务中止

在实际应用中，除了正常处理流式响应外，有时还需要能够主动中止正在进行的流式响应。例如：用户取消操作、响应内容不符合预期、达到某些业务条件（如响应长度超限、检测到敏感内容等）。

LangChain4j 提供了流式任务中止机制，允许在流式响应过程中主动取消请求。流式任务中止通过 `StreamingHandle` 实现，在以下回调方法的上下文参数中可以获取 `StreamingHandle`：

- `onPartialResponse(PartialResponse partialResponse, PartialResponseContext context)`
- `onPartialThinking(PartialThinking partialThinking, PartialThinkingContext context)`
- `onPartialToolCall(PartialToolCall partialToolCall, PartialToolCallContext context)`

调用 `StreamingHandle.cancel()` 方法即可中止流式响应。一旦调用 `cancel()`，LangChain4j 将关闭连接并停止流式传输，`StreamingChatResponseHandler` 将不再接收任何后续回调。

以下示例演示了如何实现流式任务中止，包括基于业务条件的中止（如达到最大 token 限制）和用户主动取消两种场景：

<details>
<summary>流式任务中止示例</summary>

```java
import dev.langchain4j.model.chat.StreamingChatModel;
import dev.langchain4j.model.chat.response.ChatResponse;
import dev.langchain4j.model.chat.response.PartialResponse;
import dev.langchain4j.model.chat.response.PartialResponseContext;
import dev.langchain4j.model.chat.response.StreamingChatResponseHandler;
import dev.langchain4j.model.chat.response.StreamingHandle;
import dev.langchain4j.model.openai.OpenAiStreamingChatModel;

import java.util.concurrent.CountDownLatch;
import java.util.concurrent.atomic.AtomicInteger;
import java.util.concurrent.atomic.AtomicReference;

public class StreamingCancellationExample {

    public static void main(String[] args) throws InterruptedException {
        String apiKey = System.getenv("DEEPSEEK_API_KEY");

        StreamingChatModel model = OpenAiStreamingChatModel.builder()
                .baseUrl("https://api.deepseek.com/v1")
                .apiKey(apiKey)
                .modelName("deepseek-chat")
                .build();

        String userMessage = "给我讲一个很长的故事";

        CountDownLatch latch = new CountDownLatch(1);
        AtomicInteger tokenCount = new AtomicInteger(0);
        AtomicReference<StreamingHandle> streamingHandleRef = new AtomicReference<>();
        final int MAX_TOKENS = 500; // 最大 token 数量限制

        model.chat(userMessage, new StreamingChatResponseHandler() {
            @Override
            public void onPartialResponse(PartialResponse partialResponse, PartialResponseContext context) {
                String text = partialResponse.text();
                System.out.print(text);

                // 保存 StreamingHandle 引用，供外部使用
                streamingHandleRef.compareAndSet(null, context.streamingHandle());

                // 统计已接收的 token 数量（简单示例，实际应使用更精确的 token 计数）
                int currentCount = tokenCount.addAndGet(text.length());

                // 如果达到最大 token 限制，中止流式响应
                if (currentCount >= MAX_TOKENS) {
                    System.out.println("\n\n--- 达到最大 token 限制，中止流式响应 ---");
                    context.streamingHandle().cancel();
                    latch.countDown();
                    return; // 提前返回，避免继续处理
                }
            }

            @Override
            public void onCompleteResponse(ChatResponse completeResponse) {
                System.out.println("\n\n--- 流式响应完成 ---");
                latch.countDown();
            }

            @Override
            public void onError(Throwable error) {
                System.err.println("发生错误: " + error.getMessage());
                latch.countDown();
            }
        });

        // 模拟用户操作：5秒后主动取消（如果响应还未完成）
        Thread cancelThread = new Thread(() -> {
            try {
                Thread.sleep(5000);
                StreamingHandle handle = streamingHandleRef.get();
                if (handle != null && latch.getCount() > 0) {
                    System.out.println("\n\n--- 用户主动取消流式响应 ---");
                    handle.cancel();
                    latch.countDown();
                }
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
            }
        });
        cancelThread.start();

        latch.await();
        cancelThread.join();
    }

}
```

</details>

:::warning 注意事项

- 调用 `StreamingHandle.cancel()` 后，`StreamingChatResponseHandler` 将不再接收任何后续回调（包括 `onCompleteResponse` 和 `onError`）。
- 确保在取消后及时清理相关资源（如移除 `StreamingHandle` 引用、关闭连接等）。
- 在并发场景下，注意对 `StreamingHandle` 的存储和访问进行适当的线程安全处理。
  :::

## 5. 与 Web 前端集成

流式响应最常见的应用场景是与 Web 前端集成，实现类似 ChatGPT 的实时打字效果。通过将流式响应推送到前端，用户可以即时看到模型生成的内容，大幅提升交互体验。以下提供两种典型的集成方案：

### 5.1 使用 SSE（Server-Sent Events）

<details>
<summary>Spring Boot + SSE 示例</summary>

Maven 依赖：

```xml
<dependencies>
    <!-- Spring Boot Web -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
        <version>3.5.9</version>
    </dependency>

    <!-- LangChain4j OpenAI -->
    <dependency>
        <groupId>dev.langchain4j</groupId>
        <artifactId>langchain4j-open-ai</artifactId>
        <version>1.10.0</version>
    </dependency>
</dependencies>
```

Java 代码：

```java
import dev.langchain4j.model.chat.StreamingChatModel;
import dev.langchain4j.model.chat.response.ChatResponse;
import dev.langchain4j.model.chat.response.StreamingChatResponseHandler;
import dev.langchain4j.model.openai.OpenAiStreamingChatModel;
import org.springframework.http.MediaType;
import org.springframework.web.bind.annotation.*;
import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;

import java.io.IOException;

@RestController
@RequestMapping("/api/chat")
public class StreamingChatController {

    private final StreamingChatModel model;

    public StreamingChatController() {
        String apiKey = System.getenv("DEEPSEEK_API_KEY");

        this.model = OpenAiStreamingChatModel.builder()
                .baseUrl("https://api.deepseek.com/v1")
                .apiKey(apiKey)
                .modelName("deepseek-chat")
                .build();
    }

    @CrossOrigin(origins = "*") // 调试临时配置，生产环境需限定具体跨域域名，禁止通配符
    @GetMapping(value = "/stream", produces = MediaType.TEXT_EVENT_STREAM_VALUE)
    public SseEmitter streamChat(@RequestParam(name = "message") String message) {
        System.out.println("\n--- 流式响应开始 ---");

        SseEmitter emitter = new SseEmitter(60000L); // 60秒超时

        model.chat(message, new StreamingChatResponseHandler() {
            @Override
            public void onPartialResponse(String partialResponse) {
                System.out.print(partialResponse);

                try {
                    // 发送增量 token 到前端
                    emitter.send(SseEmitter.event()
                            .name("message")
                            .data(partialResponse));
                } catch (IOException e) {
                    // 连接可能已断开，标记错误并停止后续处理
                    emitter.completeWithError(e);
                }
            }

            @Override
            public void onCompleteResponse(ChatResponse completeResponse) {
                System.out.println("\n--- 流式响应完成 ---");

                try {
                    // 发送完成事件
                    emitter.send(SseEmitter.event()
                            .name("complete")
                            .data(completeResponse.aiMessage().text()));
                    emitter.complete();
                } catch (IOException e) {
                    emitter.completeWithError(e);
                }
            }

            @Override
            public void onError(Throwable error) {
                emitter.completeWithError(error);
            }
        });

        return emitter;
    }

}
```

前端 JavaScript 示例：

```javascript
const eventSource = new EventSource("/api/chat/stream?message=给我讲个笑话");

eventSource.addEventListener("message", (event) => {
  // 实时追加增量 token
  document.getElementById("response").textContent += event.data;
});

eventSource.addEventListener("complete", (event) => {
  // 响应完成
  console.log("完整响应:", event.data);
  eventSource.close();
});

eventSource.onerror = (error) => {
  console.error("SSE 错误:", error);
  eventSource.close();
};
```

</details>

### 5.2 使用 WebSocket

<details>
<summary>Spring Boot + WebSocket 示例</summary>

Maven 依赖：

```xml
<dependencies>
    <!-- Spring Boot Web -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-web</artifactId>
        <version>3.5.9</version>
    </dependency>

    <!-- Spring Boot WebSocket -->
    <dependency>
        <groupId>org.springframework.boot</groupId>
        <artifactId>spring-boot-starter-websocket</artifactId>
        <version>3.5.9</version>
    </dependency>

    <!-- LangChain4j OpenAI -->
    <dependency>
        <groupId>dev.langchain4j</groupId>
        <artifactId>langchain4j-open-ai</artifactId>
        <version>1.10.0</version>
    </dependency>
</dependencies>
```

Java 代码：

```java
import dev.langchain4j.model.chat.StreamingChatModel;
import dev.langchain4j.model.chat.response.ChatResponse;
import dev.langchain4j.model.chat.response.StreamingChatResponseHandler;
import dev.langchain4j.model.openai.OpenAiStreamingChatModel;
import org.springframework.messaging.handler.annotation.MessageMapping;
import org.springframework.messaging.simp.SimpMessagingTemplate;
import org.springframework.stereotype.Controller;

@Controller
public class WebSocketChatController {

    private final StreamingChatModel model;
    private final SimpMessagingTemplate messagingTemplate;

    public WebSocketChatController(SimpMessagingTemplate messagingTemplate) {
        String apiKey = System.getenv("DEEPSEEK_API_KEY");

        this.model = OpenAiStreamingChatModel.builder()
                .baseUrl("https://api.deepseek.com/v1")
                .apiKey(apiKey)
                .modelName("deepseek-chat")
                .build();
        this.messagingTemplate = messagingTemplate;
    }

    @MessageMapping("/chat")
    public void handleChat(String message) {
        model.chat(message, new StreamingChatResponseHandler() {
            @Override
            public void onPartialResponse(String partialResponse) {
                // 通过 WebSocket 推送增量 token
                messagingTemplate.convertAndSend("/topic/response",
                        new StreamingMessage("partial", partialResponse));
            }

            @Override
            public void onCompleteResponse(ChatResponse completeResponse) {
                // 推送完成事件
                messagingTemplate.convertAndSend("/topic/response",
                        new StreamingMessage("complete", completeResponse.aiMessage().text()));
            }

            @Override
            public void onError(Throwable error) {
                messagingTemplate.convertAndSend("/topic/response",
                        new StreamingMessage("error", error.getMessage()));
            }
        });
    }

    // 消息封装类
    public static class StreamingMessage {

        private String type;
        private String content;

        public StreamingMessage(String type, String content) {
            this.type = type;
            this.content = content;
        }

        // getters and setters
        public String getType() {
            return type;
        }

        public void setType(String type) {
            this.type = type;
        }

        public String getContent() {
            return content;
        }

        public void setContent(String content) {
            this.content = content;
        }

    }

}
```

前端 JavaScript 示例：

```javascript
// 创建 SockJS 实例，连接后端 WebSocket 服务端点（实际部署时需替换为真实路由）
const socket = new SockJS("/ws");
// 基于 SockJS 连接创建 STOMP 客户端，用于实现 WebSocket 的消息订阅/发送
const stompClient = Stomp.over(socket);

stompClient.connect({}, function (frame) {
  console.log("WebSocket 连接成功");

  // 订阅响应主题
  stompClient.subscribe("/topic/response", function (message) {
    const data = JSON.parse(message.body);

    if (data.type === "partial") {
      // 实时追加增量 token
      document.getElementById("response").textContent += data.content;
    } else if (data.type === "complete") {
      // 响应完成
      console.log("完整响应:", data.content);
    } else if (data.type === "error") {
      // 错误处理
      console.error("错误:", data.content);
    }
  });
});

socket.onerror = function (error) {
  console.error("WebSocket 错误:", error);
};

socket.onclose = function () {
  console.log("WebSocket 连接已关闭");
};

// 发送消息
function sendMessage(message) {
  stompClient.send("/app/chat", {}, message);
}

// 示例：发送聊天消息
sendMessage("给我讲个笑话");
```

</details>

## 6. 实践建议

在实际项目中使用流式响应时，需要注意以下几个方面：

- **错误处理与资源管理**：为流式请求配置合理超时阈值，对可重试异常实现指数退避重试策略；异常场景下保证连接闭环、资源释放，同时限制流式请求并发数，避免资源耗尽。

- **幂等性与状态管理**：为每个流式请求分配唯一请求 ID，支撑全链路追踪与重复请求过滤；状态数据仅在 `onCompleteResponse` 阶段统一持久化，规避 `onPartialResponse` 高频回调的写入性能损耗；支持用户主动终止流式请求，并及时清理关联资源。

- **性能优化**：对高频触发的 `onPartialResponse` 回调采用批量聚合后推送的策略；前端侧实现缓冲机制，降低 UI 高频更新带来的渲染开销。

## 参考文档

- [LangChain4j 官方文档 - Response Streaming](https://docs.langchain4j.dev/tutorials/response-streaming)
